{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte-Pair Encoding tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the Huggingface tutorial https://huggingface.co/learn/nlp-course/chapter6/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"NLP course\",\n",
    "    \"NLP: Natural Language Processing\",\n",
    "    \"Attention is all you need.\",\n",
    "    \"Transformer: A Novel Neural Network Architecture for Language Understanding.\",\n",
    "    \"The Annotated Transformer.\",\n",
    "    \"The Illustrated Transformer.\",\n",
    "    \"Layer Normalization\",\n",
    "    \"Dropout: A Simple Way to Prevent Neural Networks from Overfitting.\",\n",
    "    \"Improving Transformer Optimization Through Better Initialization\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `bytes_to_unicode()` from: https://github.com/karpathy/minGPT/blob/master/mingpt/bpe.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Every possible byte (really an integer 0..255) gets mapped by OpenAI to a unicode\n",
    "    character that represents it visually. Some bytes have their appearance preserved\n",
    "    because they don't cause any trouble. These are defined in list bs. For example:\n",
    "    chr(33) returns \"!\", so in the returned dictionary we simply have d[33] -> \"!\".\n",
    "    However, chr(0), for example, is '\\x00', which looks ugly. So OpenAI maps these\n",
    "    bytes, into new characters in a range where chr() returns a single nice character.\n",
    "    So in the final dictionary we have d[0] -> 'Ā' instead, which is just chr(0 + 2**8).\n",
    "    In particular, the space character is 32, which we can see by ord(' '). Instead,\n",
    "    this function will shift space (32) by 256 to 288, so d[32] -> 'Ġ'.\n",
    "    So this is just a simple one-to-one mapping of bytes 0..255 into unicode characters\n",
    "    that \"look nice\", either in their original form, or a funny shifted character\n",
    "    like 'Ā', or 'Ġ', etc.\n",
    "    \"\"\"\n",
    "    # the 188 integers that render fine in their original form and need no shifting\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
    "    cs = bs[:] # all integers b in bs will simply map to chr(b) in the output dict\n",
    "    # now get the representations of the other 68 integers that do need shifting\n",
    "    # each will get mapped chr(256 + n), where n will grow from 0...67 in the loop\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            # if this byte is \"ugly\" then map it to the next available \"nice\" character\n",
    "            bs.append(b)\n",
    "            cs.append(2**8+n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    d = dict(zip(bs, cs))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will cheat a little bit and use the `transformers` package to do the pre-tokenization phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'NLP': 2, 'Ġcourse': 1, ':': 3, 'ĠNatural': 1, 'ĠLanguage': 2, 'ĠProcessing': 1, 'Attention': 1, 'Ġis': 1, 'Ġall': 1, 'Ġyou': 1, 'Ġneed': 1, '.': 5, 'Transformer': 1, 'ĠA': 2, 'ĠNovel': 1, 'ĠNeural': 2, 'ĠNetwork': 1, 'ĠArchitecture': 1, 'Ġfor': 1, 'ĠUnderstanding': 1, 'The': 2, 'ĠAnnotated': 1, 'ĠTransformer': 3, 'ĠIllustrated': 1, 'Layer': 1, 'ĠNormalization': 1, 'Dropout': 1, 'ĠSimple': 1, 'ĠWay': 1, 'Ġto': 1, 'ĠPrevent': 1, 'ĠNetworks': 1, 'Ġfrom': 1, 'ĠOverfitting': 1, 'Improving': 1, 'ĠOptimization': 1, 'ĠThrough': 1, 'ĠBetter': 1, 'ĠInitialization': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_freqs = defaultdict(int)\n",
    "\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [word for word, offset in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] += 1\n",
    "\n",
    "print(word_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', ':', 'A', 'B', 'D', 'I', 'L', 'N', 'O', 'P', 'S', 'T', 'U', 'W', 'a', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ġ']\n"
     ]
    }
   ],
   "source": [
    "alphabet = list(\n",
    "    set([letter \n",
    "            for word in word_freqs.keys() \n",
    "                 for letter in word\n",
    "        ])\n",
    "    )\n",
    "\n",
    "alphabet.sort()\n",
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', '.', ':', 'A', 'B', 'D', 'I', 'L', 'N', 'O', 'P', 'S', 'T', 'U', 'W', 'a', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ġ']\n"
     ]
    }
   ],
   "source": [
    "vocab = [\"<|endoftext|>\"] + alphabet.copy()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE \"training\" step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NLP': ['N', 'L', 'P'], 'Ġcourse': ['Ġ', 'c', 'o', 'u', 'r', 's', 'e'], ':': [':'], 'ĠNatural': ['Ġ', 'N', 'a', 't', 'u', 'r', 'a', 'l'], 'ĠLanguage': ['Ġ', 'L', 'a', 'n', 'g', 'u', 'a', 'g', 'e'], 'ĠProcessing': ['Ġ', 'P', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g'], 'Attention': ['A', 't', 't', 'e', 'n', 't', 'i', 'o', 'n'], 'Ġis': ['Ġ', 'i', 's'], 'Ġall': ['Ġ', 'a', 'l', 'l'], 'Ġyou': ['Ġ', 'y', 'o', 'u'], 'Ġneed': ['Ġ', 'n', 'e', 'e', 'd'], '.': ['.'], 'Transformer': ['T', 'r', 'a', 'n', 's', 'f', 'o', 'r', 'm', 'e', 'r'], 'ĠA': ['Ġ', 'A'], 'ĠNovel': ['Ġ', 'N', 'o', 'v', 'e', 'l'], 'ĠNeural': ['Ġ', 'N', 'e', 'u', 'r', 'a', 'l'], 'ĠNetwork': ['Ġ', 'N', 'e', 't', 'w', 'o', 'r', 'k'], 'ĠArchitecture': ['Ġ', 'A', 'r', 'c', 'h', 'i', 't', 'e', 'c', 't', 'u', 'r', 'e'], 'Ġfor': ['Ġ', 'f', 'o', 'r'], 'ĠUnderstanding': ['Ġ', 'U', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd', 'i', 'n', 'g'], 'The': ['T', 'h', 'e'], 'ĠAnnotated': ['Ġ', 'A', 'n', 'n', 'o', 't', 'a', 't', 'e', 'd'], 'ĠTransformer': ['Ġ', 'T', 'r', 'a', 'n', 's', 'f', 'o', 'r', 'm', 'e', 'r'], 'ĠIllustrated': ['Ġ', 'I', 'l', 'l', 'u', 's', 't', 'r', 'a', 't', 'e', 'd'], 'Layer': ['L', 'a', 'y', 'e', 'r'], 'ĠNormalization': ['Ġ', 'N', 'o', 'r', 'm', 'a', 'l', 'i', 'z', 'a', 't', 'i', 'o', 'n'], 'Dropout': ['D', 'r', 'o', 'p', 'o', 'u', 't'], 'ĠSimple': ['Ġ', 'S', 'i', 'm', 'p', 'l', 'e'], 'ĠWay': ['Ġ', 'W', 'a', 'y'], 'Ġto': ['Ġ', 't', 'o'], 'ĠPrevent': ['Ġ', 'P', 'r', 'e', 'v', 'e', 'n', 't'], 'ĠNetworks': ['Ġ', 'N', 'e', 't', 'w', 'o', 'r', 'k', 's'], 'Ġfrom': ['Ġ', 'f', 'r', 'o', 'm'], 'ĠOverfitting': ['Ġ', 'O', 'v', 'e', 'r', 'f', 'i', 't', 't', 'i', 'n', 'g'], 'Improving': ['I', 'm', 'p', 'r', 'o', 'v', 'i', 'n', 'g'], 'ĠOptimization': ['Ġ', 'O', 'p', 't', 'i', 'm', 'i', 'z', 'a', 't', 'i', 'o', 'n'], 'ĠThrough': ['Ġ', 'T', 'h', 'r', 'o', 'u', 'g', 'h'], 'ĠBetter': ['Ġ', 'B', 'e', 't', 't', 'e', 'r'], 'ĠInitialization': ['Ġ', 'I', 'n', 'i', 't', 'i', 'a', 'l', 'i', 'z', 'a', 't', 'i', 'o', 'n']}\n"
     ]
    }
   ],
   "source": [
    "splits = {word: [c for c in word] for word in word_freqs.keys()}\n",
    "print(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def sample_from_dict(d, sample=10):\n",
    "    keys = random.sample(list(d), sample)\n",
    "    values = [d[k] for k in keys]\n",
    "    return dict(zip(keys, values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pair_freqs(splits):\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            pair_freqs[pair] += freq\n",
    "    return pair_freqs\n",
    "\n",
    "pair_freqs = compute_pair_freqs(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('e', 'n'): 2, ('e', 'd'): 3, ('l', 'u'): 1, ('e', 'r'): 8, ('Ġ', 'c'): 1, ('i', 't'): 3, ('c', 'o'): 1, ('f', 'i'): 1, ('u', 'a'): 2, ('I', 'l'): 1}\n"
     ]
    }
   ],
   "source": [
    "print(sample_from_dict(pair_freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('r', 'a') 8\n"
     ]
    }
   ],
   "source": [
    "best_pair = max(pair_freqs, key=pair_freqs.get)\n",
    "print(best_pair, pair_freqs[best_pair])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('r', 'a'): 'ra'}\n",
      "['<|endoftext|>', '.', ':', 'A', 'B', 'D', 'I', 'L', 'N', 'O', 'P', 'S', 'T', 'U', 'W', 'a', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ġ', 'ra']\n"
     ]
    }
   ],
   "source": [
    "merges = { best_pair: best_pair[0] + best_pair[1]}\n",
    "vocab.append(best_pair[0] + best_pair[1])\n",
    "print(merges)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                split = split[:i] + [a + b] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ġ', 'T', 'r', 'a', 'n', 's', 'f', 'or', 'm', 'e', 'r']\n"
     ]
    }
   ],
   "source": [
    "splits = merge_pair('o','r',splits)\n",
    "print(splits['ĠTransformer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50\n",
    "\n",
    "while len(vocab) < vocab_size:\n",
    "    pair_freqs = compute_pair_freqs(splits)\n",
    "    for pair, freq in pair_freqs.items():\n",
    "        best_pair = max(pair_freqs, key=pair_freqs.get)\n",
    "        splits = merge_pair(*best_pair, splits)\n",
    "    merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "    vocab.append(best_pair[0] + best_pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('r', 'a'): 'ra', ('e', 'r'): 'er', ('Ġ', 'N'): 'ĠN', ('t', 'i'): 'ti', ('n', 'g'): 'ng', ('r', 'o'): 'ro', ('f', 'or'): 'for', ('t', 'e'): 'te', ('ti', 'o'): 'tio', ('tio', 'n'): 'tion', ('T', 'ra'): 'Tra'}\n"
     ]
    }
   ],
   "source": [
    "print(merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', '.', ':', 'A', 'B', 'D', 'I', 'L', 'N', 'O', 'P', 'S', 'T', 'U', 'W', 'a', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ġ', 'ra', 'ra', 'er', 'ĠN', 'ti', 'ng', 'ro', 'for', 'te', 'tio', 'tion', 'Tra']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "    splits = [[l for l in word] for word in pre_tokenized_text]\n",
    "    for pair, merge in merges.items():\n",
    "        for idx, split in enumerate(splits):\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                    split = split[:i] + [merge] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            splits[idx] = split\n",
    "\n",
    "    return sum(splits, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 't', 'te', 'n', 'tion', 'i', 's', 'a', 'l', 'l', 'y', 'o', 'u', 'n', 'e', 'e', 'd']\n",
      "['a', 't', 'te', 'n', 'tion', 'm', 'a', 's', 't', 'er', 's']\n",
      "['Tra', 'n', 's', 'f', 'o', 'r', 'm', 'er', 's', 'C', 'h', 'a', 't', 'b', 'o', 't', 's', 'I', 'n', 'D', 'i', 's', 'g', 'u', 'i', 's', 'e']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(\"attentionisallyouneed\"))\n",
    "print(tokenize(\"attentionmasters\"))\n",
    "print(tokenize(\"TransformersChatbotsInDisguise\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 't', 'te', 'n', 'tion', 'i', 's', 'a', 'l', 'l', 'y', 'o', 'u', 'n', 'e', 'e', 'd']\n",
      "[0, 1, 2, 3, 4, 5, 6, 0, 8, 8, 10, 11, 12, 3, 14, 14, 16]\n"
     ]
    }
   ],
   "source": [
    "def tokens_to_index(tokens):\n",
    "    idx_list = []\n",
    "    for tok in tokens:\n",
    "        idx_list.append(tokens.index(tok))\n",
    "    return idx_list\n",
    "\n",
    "print(tokenize(\"attentionisallyouneed\"))\n",
    "print(tokens_to_index(tokenize(\"attentionisallyouneed\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        width:800px;\n",
       "        font-size: 110%;\n",
       "        margin-left:5% !important;\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: Helvetica, serif;\n",
       "    }\n",
       "    h4{\n",
       "        margin-top:12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "    div.text_cell_render{\n",
       "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 145%;\n",
       "        font-size: 110%;\n",
       "        width:800px;\n",
       "        margin-left:auto;\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
       "            font-size: 110%;\n",
       "    }\n",
       "    .prompt{\n",
       "        display: None;\n",
       "    }\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 22pt;\n",
       "        color: #4057A1;\n",
       "        font-style: italic;\n",
       "        margin-bottom: .5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "    \n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }  \n",
       "\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "def css_styling():\n",
    "    styles = open(\"../css/notebook.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
