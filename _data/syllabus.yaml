- title: "Course Introduction"
  tag: intro
  include: true
  current: false
  notes:
    - title: "Alignment Exercise"
      author: "Kevin Knight"
      url: "assets/slides/centauri-arcturan-align.pdf"
    - title: "Language Models"
      url: "assets/slides/lm.pdf"
  links:
    - author: "Various Allen AI researchers"
      title: "NLP Highlights podcast"
      url: "https://soundcloud.com/nlp-highlights"
      optional: true
- title: "Cross Attention"
  tag: cross_attention
  include: true
  current: false
  notes:
    - title: "Lecture notes"
      url: "assets/slides/cross_attention.pdf"
    - title: "Neural Machine Translation by Jointly Learning to Align and Translate"
      author: "Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio"
      url: "https://arxiv.org/abs/1409.0473"
  links:
    - title: "Sequence to Sequence Learning with Neural Networks"
      author: "Ilya Sutskever, Oriol Vinyals, Quoc V. Le"
      url: "https://arxiv.org/abs/1409.3215"
    - title: "Effective Approaches to Attention-based Neural Machine Translation"
      author: "Minh-Thang Luong, Hieu Pham, Christopher D. Manning"
      url: "https://arxiv.org/abs/1508.04025"
- title: "Self Attention"
  tag: self_attention
  include: true
  current: false
  notes:
    - title: "Lecture notes"
      url: "assets/slides/self_attention.pdf"
    - title: "Attention is all you need"
      author: "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
      url: "https://arxiv.org/abs/1706.03762"
    - title: "Transformer: A Novel Neural Network Architecture for Language Understanding"
      author: "posted by Jakob Uszkoreit"
      url: "https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html"
    - title: "Stanford cs224n lecture notes on self attention"
      author: "John Hewitt"
      url: "https://web.stanford.edu/class/cs224n/readings/cs224n-self-attention-transformers-2023_draft.pdf"
    - title: "The Annotated Transformer"
      author: "Austin Huang, Suraj Subramanian, Jonathan Sum, Khalid Almubarak, and Stella Biderman. Original by Sasha Rush"
      url: "http://nlp.seas.harvard.edu/annotated-transformer/#positional-encoding"
    - title: "The Illustrated Transformer"
      author: "Jay Alammar"
      url: "http://jalammar.github.io/illustrated-transformer/"
  links:
    - title: "Convolutional Sequence to Sequence Learning"
      author: "Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann N. Dauphin"
      url: "https://arxiv.org/abs/1705.03122"
    - title: "Layer Normalization"
      author: "Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton"
      url: "https://arxiv.org/abs/1607.06450"
    - title: "Dropout: A Simple Way to Prevent Neural Networks from Overfitting"
      author: "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov"
      url: "https://jmlr.org/papers/v15/srivastava14a.html"
    - title: "Improving Transformer Optimization Through Better Initialization"
      author: "Xiao Shi Huang, Felipe Perez, Jimmy Ba, Maksims Volkovs"
      url: "https://proceedings.mlr.press/v119/huang20f.html"
    - title: "A Mathematical Framework for Transformer Circuits"
      author: "Anthropic"
      url: "https://transformer-circuits.pub/2021/framework/index.html"
    - title: "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"
      author: "Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, Ivan Titov"
      url: "https://aclanthology.org/P19-1580/"
- title: "Pre-training Transformers"
  tag: "bert"
  include: true
  current: false
  notes:
    - title: Lecture notes
      url: "assets/slides/pre_training.pdf"
    - title: "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      author: "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
      url: "https://arxiv.org/abs/1810.04805"
    - title: "Improving language understanding with unsupervised learning"
      author: "Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever"
      url: "https://openai.com/research/language-unsupervised"
  links:
    - title: "Semi-supervised Sequence Learning"
      author: "Andrew M. Dai, Quoc V. Le"
      url: "https://arxiv.org/abs/1511.01432"
    - title: "Deep contextualized word representations"
      author: "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer"
      url: "https://arxiv.org/abs/1802.05365"
    - title: "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
      author: "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov"
      url: "https://arxiv.org/abs/1907.11692"
    - title: "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"
      author: "Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov"
      url: "https://arxiv.org/abs/1901.02860"
    - title: "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"
      author: "Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning"
      url: "https://arxiv.org/abs/2003.10555"
    - title: "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
      author: "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu"
      url: "https://arxiv.org/abs/1910.10683"
    - title: "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"
      author: "Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut"
      url: "https://arxiv.org/abs/1909.11942"
- title: "Benchmark datasets"
  tag: "benchmarks"
  include: true
  current: false
  notes:
    - title: Lecture notes
      url: "assets/slides/benchmarks.pdf"
- title: "Model Compression"
  tag: "compression"
  include: true
  current: false
  notes:
    - title: Lecture notes
      url: "assets/slides/compression.pdf"
    - title: "Distilling the Knowledge in a Neural Network"
      author: "Geoffrey Hinton, Oriol Vinyals, Jeff Dean"
      url: "https://arxiv.org/abs/1503.02531"
  links:
    - title: "Sequence-Level Knowledge Distillation"
      author: "Yoon Kim, Alexander Rush"
      url: "https://nlp.seas.harvard.edu/slides/emnlp16_seqkd.pdf"
    - title: "Dark Knowledge"
      author: "Geoffrey Hinton"
      url: "https://www.ttic.edu/dl/dark14.pdf"
    - title: "DistilBERT"
      author: "Huggingface"
      url: "https://medium.com/huggingface/distilbert-8cf3380435b5"
    - title: "Compressing Large-Scale Transformer-Based Models: A Case Study on BERT"
      author: "Prakhar Ganesh,  Yao Chen,  Xin Lou,  Mohammad Ali Khan,  Yin Yang,  Hassan Sajjad,  Preslav Nakov,  Deming Chen,  Marianne Winslett"
      url: "https://doi.org/10.1162/tacl_a_00413"
- title: "Parameter-efficient Fine Tuning"
  tag: "peft"
  include: true
  current: false
  notes:
    - title: Lecture notes
      url: "assets/slides/peft.pdf"
    - title: "Prefix-Tuning: Optimizing Continuous Prompts for Generation"
      author: "Xiang Lisa Li, Percy Liang"
      url: "https://arxiv.org/abs/2101.00190"
    - title: "AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning"
      author: "Yaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan Awadallah, Jianfeng Gao"
      url: "https://arxiv.org/abs/2205.12410"
    - title: "LoRA: Low-Rank Adaptation of Large Language Models"
      author: "Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen"
      url: "https://arxiv.org/abs/2106.09685"
    - title: "Adapter methods"
      author: "docs.adapterhub.ml"
      url: "https://docs.adapterhub.ml/overview.html"
  links:
    - title: "AdapterHub: A Framework for Adapting Transformers"
      author: "Jonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya Kamath, Ivan Vulić, Sebastian Ruder, Kyunghyun Cho, Iryna Gurevych"
      url: "https://aclanthology.org/2020.emnlp-demos.7/"
    - title: "Parameter-Efficient Transfer Learning for NLP"
      author: "Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly"
      url: "https://arxiv.org/abs/1902.00751"
    - title: "Simple, Scalable Adaptation for Neural Machine Translation"
      author: "Ankur Bapna, Naveen Arivazhagan, Orhan Firat"
      url: "https://arxiv.org/abs/1909.08478"
    - title: "AdapterFusion: Non-Destructive Task Composition for Transfer Learning"
      author: "Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, Iryna Gurevych"
      url: "https://aclanthology.org/2021.eacl-main.39/"
    - title: "Parameter-Efficient Tuning with Special Token Adaptation"
      author: "Xiaocong Yang, James Y. Huang, Wenxuan Zhou, Muhao Chen"
      url: "https://aclanthology.org/2023.eacl-main.60/"
- title: "Few-shot learning"
  tag: "fewshot"
  include: true
  current: true
  notes:
    - title: Lecture notes
      url: "assets/slides/fewshot.pdf"
    - title: "Scaling Laws for Neural Language Models"
      author: "Open AI"
      url: "https://arxiv.org/abs/2001.08361"
    - title: "Training Compute-Optimal Large Language Models"
      author: "DeepMind"
      url: "https://arxiv.org/abs/2203.15556"
    - title: "Language Models are Unsupervised Multitask Learners"
      author: "Open AI"
      url: "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"
    - title: "Language Models are Few-Shot Learners"
      author: "Open AI"
      url: "https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf"
    - title: "GPT-4 Technical Report"
      author: "Open AI"
      url: "https://arxiv.org/abs/2303.08774"
  links:
    - title: "The AI Brick Wall – A Practical Limit For Scaling Dense Transformer Models, and How GPT 4 Will Break Past It"
      author: "Dylan Patel"
      url: "https://www.semianalysis.com/p/the-ai-brick-wall-a-practical-limit"
    - title: "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?"
      author: "Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer"
      url: "https://arxiv.org/abs/2202.12837"
- title: "Nearest Neighbour Language Models"
  tag: "knnlm"
  include: true
  current: false
  notes:
   - title: Lecture notes
     url: "assets/slides/knnlm.pdf"
   - title: "Generalization through Memorization: Nearest Neighbor Language Models"
     author: "Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, Mike Lewis"
     url: "https://openreview.net/forum?id=HklBjCEKvH"
- title: "Fast Attention"
  tag: "fastattn"
  include: true
  current: false
  notes:
   - title: "Reformer: The Efficient Transformer"
     author: "Nikita Kitaev, Lukasz Kaiser, Anselm Levskaya"
     url: "https://openreview.net/forum?id=rkgNKkHtvB"
   - title: "Rethinking Attention with Performers"
     author: "Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, Adrian Weller"
     url: "https://openreview.net/forum?id=Ua6zuk0WRH"
- title: "Data Efficiency"
  tag: "data"
  include: true
  current: false
  notes:
   - title: "Do We Need to Create Big Datasets to Learn a Task?"
     author: "Swaroop Mishra, Bhavdeep Singh Sachdeva"
     url: "https://aclanthology.org/2020.sustainlp-1.23"
   - title: "Shortformer: Better Language Modeling using Shorter Inputs"
     author: "Ofir Press, Noah A. Smith, Mike Lewis"
     url: "https://aclanthology.org/2021.acl-long.427/"
   - title: "Active Learning for BERT: An Empirical Study"
     author: "Liat Ein-Dor, Alon Halfon, Ariel Gera, Eyal Shnarch, Lena Dankin, Leshem Choshen, Marina Danilevsky, Ranit Aharonov, Yoav Katz, Noam Slonim"
     url: "https://aclanthology.org/2020.emnlp-main.638/"
- title: "Context Length"
  tag: "longcontext"
  include: true
  current: false
  notes:
   - title: "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"
     author: "Ofir Press, Noah A. Smith, Mike Lewis"
     url: "https://arxiv.org/abs/2108.12409"
   - title: "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"
     author: "Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, Ruslan Salakhutdinov"
     url: "https://aclanthology.org/P19-1285/"
   - title: "∞-former: Infinite Memory Transformer"
     author: "Pedro Henrique Martins, Zita Marinho, Andre Martins"
     url: "https://aclanthology.org/2022.acl-long.375/"
- title: "Summary"
  tag: "summary"
  include: true
  current: false
  notes:
   - title: "Efficient Methods for Natural Language Processing: A Survey"
     author: "Marcos Treviso, Ji-Ung Lee, Tianchu Ji, Betty van Aken, Qingqing Cao, Manuel R. Ciosici, Michael Hassid, Kenneth Heafield, Sara Hooker, Colin Raffel, Pedro H. Martins, André F. T. Martins, Jessica Zosa Forde, Peter Milder, Edwin Simpson, Noam Slonim, Jesse Dodge, Emma Strubell, Niranjan Balasubramanian, Leon Derczynski, Iryna Gurevych, Roy Schwartz"
     url: "https://arxiv.org/abs/2209.00099"
