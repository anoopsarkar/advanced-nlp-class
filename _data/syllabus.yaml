- title: "Course Introduction"
  tag: intro
  include: true
  current: false
  notes:
    - title: "Alignment Exercise"
      author: "Kevin Knight"
      url: "assets/slides/centauri-arcturan-align.pdf"
    - title: "Language Models"
      url: "assets/slides/lm.pdf"
  links:
    - author: "Various Allen AI researchers"
      title: "NLP Highlights podcast"
      url: "https://soundcloud.com/nlp-highlights"
      optional: true
- title: "Cross Attention"
  tag: cross_attention
  include: true
  current: false
  notes:
    - title: "Neural Machine Translation by Jointly Learning to Align and Translate"
      author: "Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio"
      url: "https://arxiv.org/abs/1409.0473"
    - title: "Lecture notes"
      url: "assets/slides/cross_attention.pdf"
  links:
    - title: "Sequence to Sequence Learning with Neural Networks"
      author: "Ilya Sutskever, Oriol Vinyals, Quoc V. Le"
      url: "https://arxiv.org/abs/1409.3215"
    - title: "Effective Approaches to Attention-based Neural Machine Translation"
      author: "Minh-Thang Luong, Hieu Pham, Christopher D. Manning"
      url: "https://arxiv.org/abs/1508.04025"
- title: "Self Attention"
  tag: self_attention
  include: true
  current: false
  notes:
    - title: "Attention is all you need"
      author: "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
      url: "https://arxiv.org/abs/1706.03762"
    - title: "Transformer: A Novel Neural Network Architecture for Language Understanding"
      author: "posted by Jakob Uszkoreit"
      url: "https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html"
    - title: "Lecture notes"
      url: "assets/slides/self_attention.pdf"
    - title: "Stanford cs224n lecture notes on self attention"
      author: "John Hewitt"
      url: "https://web.stanford.edu/class/cs224n/readings/cs224n-self-attention-transformers-2023_draft.pdf"
    - title: "The Annotated Transformer"
      author: "Austin Huang, Suraj Subramanian, Jonathan Sum, Khalid Almubarak, and Stella Biderman. Original by Sasha Rush"
      url: "http://nlp.seas.harvard.edu/annotated-transformer/#positional-encoding"
    - title: "The Illustrated Transformer"
      author: "Jay Alammar"
      url: "http://jalammar.github.io/illustrated-transformer/"
  links:
    - title: "Convolutional Sequence to Sequence Learning"
      author: "Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann N. Dauphin"
      url: "https://arxiv.org/abs/1705.03122"
    - title: "Layer Normalization"
      author: "Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton"
      url: "https://arxiv.org/abs/1607.06450"
    - title: "Dropout: A Simple Way to Prevent Neural Networks from Overfitting"
      author: "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov"
      url: "https://jmlr.org/papers/v15/srivastava14a.html"
    - title: "Improving Transformer Optimization Through Better Initialization"
      author: "Xiao Shi Huang, Felipe Perez, Jimmy Ba, Maksims Volkovs"
      url: "https://proceedings.mlr.press/v119/huang20f.html"
- title: "Pre-training Transformers"
  tag: "bert"
  include: true
  current: true
  notes:
    - title: "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      author: "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
      url: "https://arxiv.org/abs/1810.04805"
    - title: "Improving language understanding with unsupervised learning"
      author: "Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever"
      url: "https://openai.com/research/language-unsupervised"
    - title: Lecture notes
      url: "assets/slides/pre_training.pdf"
  links:
    - title: "Semi-supervised Sequence Learning"
      author: "Andrew M. Dai, Quoc V. Le"
      url: "https://arxiv.org/abs/1511.01432"
    - title: "Deep contextualized word representations"
      author: "Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer"
      url: "https://arxiv.org/abs/1802.05365"
- title: "Parameter-efficient Fine Tuning"
  tag: "peft"
  include: true
  current: false
  notes:
    - title: "Prefix-Tuning: Optimizing Continuous Prompts for Generation"
      author: "Xiang Lisa Li, Percy Liang"
      url: "https://arxiv.org/abs/2101.00190"
    - title: "AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning"
      author: "Yaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan Awadallah, Jianfeng Gao"
      url: "https://arxiv.org/abs/2205.12410"
    - title: "Adapter methods"
      author: "docs.adapterhub.ml"
      url: "https://docs.adapterhub.ml/overview.html"
  links:
    - title: "AdapterHub: A Framework for Adapting Transformers"
      author: "Jonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya Kamath, Ivan Vulić, Sebastian Ruder, Kyunghyun Cho, Iryna Gurevych"
      url: "https://aclanthology.org/2020.emnlp-demos.7/"
